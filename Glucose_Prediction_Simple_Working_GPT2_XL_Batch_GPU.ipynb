{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNhnDc6iBlv5mKx3hXtp89c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yshin1209/Awesome-CGM/blob/master/Glucose_Prediction_Simple_Working_GPT2_XL_Batch_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train empty GPT-2 for glucose level prediction."
      ],
      "metadata": {
        "id": "SRy-TPgEl6To"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8y4sRLmkfL0",
        "outputId": "09f2a648-ed5d-4b32-a2f8-8f99565f8a21"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.26.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n"
      ],
      "metadata": {
        "id": "l9ka9-gfJksI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "J8bughaFl4rx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ql6Qk-askW2q",
        "outputId": "dd71fadc-4069-43f4-96f9-cdd77a3fb467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_loss b: 0\n",
            "total_loss a: 0\n",
            "total_loss b: 11.139847755432129\n",
            "total_loss a: 11.139847755432129\n",
            "total_loss b: 20.84402561187744\n",
            "total_loss a: 20.84402561187744\n",
            "total_loss b: 29.676016807556152\n",
            "total_loss a: 29.676016807556152\n",
            "total_loss b: 38.19645881652832\n",
            "total_loss a: 38.19645881652832\n",
            "total_loss b: 46.78644847869873\n",
            "total_loss a: 46.78644847869873\n",
            "total_loss b: 55.02399253845215\n",
            "total_loss a: 55.02399253845215\n",
            "total_loss b: 63.32712936401367\n",
            "total_loss a: 63.32712936401367\n",
            "total_loss b: 71.4927568435669\n",
            "total_loss a: 71.4927568435669\n",
            "total_loss b: 78.94521903991699\n",
            "total_loss a: 78.94521903991699\n",
            "total_loss b: 86.71383333206177\n",
            "total_loss a: 86.71383333206177\n",
            "total_loss b: 94.33285570144653\n",
            "total_loss a: 94.33285570144653\n",
            "total_loss b: 101.08673477172852\n",
            "total_loss a: 101.08673477172852\n",
            "total_loss b: 108.00678062438965\n",
            "total_loss a: 108.00678062438965\n",
            "total_loss b: 114.80997943878174\n",
            "total_loss a: 114.80997943878174\n",
            "total_loss b: 121.7494592666626\n",
            "total_loss a: 121.7494592666626\n",
            "total_loss b: 128.1883840560913\n",
            "total_loss a: 128.1883840560913\n",
            "total_loss b: 134.3837742805481\n",
            "total_loss a: 134.3837742805481\n",
            "total_loss b: 140.4892120361328\n",
            "total_loss a: 140.4892120361328\n",
            "total_loss b: 146.50878858566284\n",
            "total_loss a: 146.50878858566284\n",
            "total_loss b: 152.3257155418396\n",
            "total_loss a: 152.3257155418396\n",
            "total_loss b: 158.1482195854187\n",
            "total_loss a: 158.1482195854187\n",
            "total_loss b: 163.630624294281\n",
            "total_loss a: 163.630624294281\n",
            "total_loss b: 169.12269926071167\n",
            "total_loss a: 169.12269926071167\n",
            "total_loss b: 174.53529357910156\n",
            "total_loss a: 174.53529357910156\n",
            "total_loss b: 180.06161546707153\n",
            "total_loss a: 180.06161546707153\n",
            "total_loss b: 185.78524112701416\n",
            "total_loss a: 185.78524112701416\n",
            "total_loss b: 191.42988777160645\n",
            "total_loss a: 191.42988777160645\n",
            "total_loss b: 196.86396265029907\n",
            "total_loss a: 196.86396265029907\n",
            "total_loss b: 202.43168354034424\n",
            "total_loss a: 202.43168354034424\n",
            "total_loss b: 207.875346660614\n",
            "total_loss a: 207.875346660614\n",
            "total_loss b: 213.13766813278198\n",
            "total_loss a: 213.13766813278198\n",
            "total_loss b: 218.38312244415283\n",
            "total_loss a: 218.38312244415283\n",
            "total_loss b: 223.5171890258789\n",
            "total_loss a: 223.5171890258789\n",
            "total_loss b: 229.078715801239\n",
            "total_loss a: 229.078715801239\n",
            "total_loss b: 234.51392269134521\n",
            "total_loss a: 234.51392269134521\n",
            "total_loss b: 239.84583711624146\n",
            "total_loss a: 239.84583711624146\n",
            "total_loss b: 245.26763725280762\n",
            "total_loss a: 245.26763725280762\n",
            "total_loss b: 250.8060073852539\n",
            "total_loss a: 250.8060073852539\n",
            "total_loss b: 256.4533495903015\n",
            "total_loss a: 256.4533495903015\n",
            "total_loss b: 261.7933712005615\n",
            "total_loss a: 261.7933712005615\n",
            "total_loss b: 267.27399015426636\n",
            "total_loss a: 267.27399015426636\n",
            "total_loss b: 272.41632175445557\n",
            "total_loss a: 272.41632175445557\n",
            "total_loss b: 277.8481979370117\n",
            "total_loss a: 277.8481979370117\n",
            "total_loss b: 283.10934352874756\n",
            "total_loss a: 283.10934352874756\n",
            "total_loss b: 288.34820556640625\n",
            "total_loss a: 288.34820556640625\n",
            "total_loss b: 293.6225652694702\n",
            "total_loss a: 293.6225652694702\n",
            "total_loss b: 298.83002185821533\n",
            "total_loss a: 298.83002185821533\n",
            "total_loss b: 304.14501428604126\n",
            "total_loss a: 304.14501428604126\n",
            "total_loss b: 309.3041400909424\n",
            "total_loss a: 309.3041400909424\n",
            "total_loss b: 314.9185400009155\n",
            "total_loss a: 314.9185400009155\n",
            "total_loss b: 320.3345546722412\n",
            "total_loss a: 320.3345546722412\n",
            "total_loss b: 325.725302696228\n",
            "total_loss a: 325.725302696228\n",
            "total_loss b: 331.12858867645264\n",
            "total_loss a: 331.12858867645264\n",
            "total_loss b: 336.7009515762329\n",
            "total_loss a: 336.7009515762329\n",
            "total_loss b: 342.11019802093506\n",
            "total_loss a: 342.11019802093506\n",
            "total_loss b: 347.37041568756104\n",
            "total_loss a: 347.37041568756104\n",
            "total_loss b: 352.72127628326416\n",
            "total_loss a: 352.72127628326416\n",
            "Epoch 1 loss: 6.17349414989866\n",
            "total_loss b: 0\n",
            "total_loss a: 0\n",
            "total_loss b: 5.54306173324585\n",
            "total_loss a: 5.54306173324585\n",
            "total_loss b: 10.82909870147705\n",
            "total_loss a: 10.82909870147705\n",
            "total_loss b: 16.094762325286865\n",
            "total_loss a: 16.094762325286865\n",
            "total_loss b: 21.39763641357422\n",
            "total_loss a: 21.39763641357422\n",
            "total_loss b: 26.77480173110962\n",
            "total_loss a: 26.77480173110962\n",
            "total_loss b: 32.16155481338501\n",
            "total_loss a: 32.16155481338501\n",
            "total_loss b: 37.76741123199463\n",
            "total_loss a: 37.76741123199463\n",
            "total_loss b: 43.47440576553345\n",
            "total_loss a: 43.47440576553345\n",
            "total_loss b: 48.755377769470215\n",
            "total_loss a: 48.755377769470215\n",
            "total_loss b: 54.35881805419922\n",
            "total_loss a: 54.35881805419922\n",
            "total_loss b: 60.10746955871582\n",
            "total_loss a: 60.10746955871582\n",
            "total_loss b: 65.34933423995972\n",
            "total_loss a: 65.34933423995972\n",
            "total_loss b: 70.83609247207642\n",
            "total_loss a: 70.83609247207642\n",
            "total_loss b: 76.26732969284058\n",
            "total_loss a: 76.26732969284058\n",
            "total_loss b: 81.88856744766235\n",
            "total_loss a: 81.88856744766235\n",
            "total_loss b: 87.24651050567627\n",
            "total_loss a: 87.24651050567627\n",
            "total_loss b: 92.52726888656616\n",
            "total_loss a: 92.52726888656616\n",
            "total_loss b: 97.83318710327148\n",
            "total_loss a: 97.83318710327148\n",
            "total_loss b: 103.23897409439087\n",
            "total_loss a: 103.23897409439087\n",
            "total_loss b: 108.46100997924805\n",
            "total_loss a: 108.46100997924805\n",
            "total_loss b: 113.76003694534302\n",
            "total_loss a: 113.76003694534302\n",
            "total_loss b: 118.93138217926025\n",
            "total_loss a: 118.93138217926025\n",
            "total_loss b: 124.09118700027466\n",
            "total_loss a: 124.09118700027466\n",
            "total_loss b: 129.26047229766846\n",
            "total_loss a: 129.26047229766846\n",
            "total_loss b: 134.51964950561523\n",
            "total_loss a: 134.51964950561523\n",
            "total_loss b: 139.9863986968994\n",
            "total_loss a: 139.9863986968994\n",
            "total_loss b: 145.4039216041565\n",
            "total_loss a: 145.4039216041565\n",
            "total_loss b: 150.6978096961975\n",
            "total_loss a: 150.6978096961975\n",
            "total_loss b: 156.07618522644043\n",
            "total_loss a: 156.07618522644043\n",
            "total_loss b: 161.36946487426758\n",
            "total_loss a: 161.36946487426758\n",
            "total_loss b: 166.52421808242798\n",
            "total_loss a: 166.52421808242798\n",
            "total_loss b: 171.66525316238403\n",
            "total_loss a: 171.66525316238403\n",
            "total_loss b: 176.72506141662598\n",
            "total_loss a: 176.72506141662598\n",
            "total_loss b: 182.180561542511\n",
            "total_loss a: 182.180561542511\n",
            "total_loss b: 187.53600120544434\n",
            "total_loss a: 187.53600120544434\n",
            "total_loss b: 192.80414295196533\n",
            "total_loss a: 192.80414295196533\n",
            "total_loss b: 198.17139196395874\n",
            "total_loss a: 198.17139196395874\n",
            "total_loss b: 203.64319229125977\n",
            "total_loss a: 203.64319229125977\n",
            "total_loss b: 209.21360397338867\n",
            "total_loss a: 209.21360397338867\n",
            "total_loss b: 214.52287817001343\n",
            "total_loss a: 214.52287817001343\n",
            "total_loss b: 219.9529013633728\n",
            "total_loss a: 219.9529013633728\n",
            "total_loss b: 225.0405683517456\n",
            "total_loss a: 225.0405683517456\n",
            "total_loss b: 230.43396949768066\n",
            "total_loss a: 230.43396949768066\n",
            "total_loss b: 235.68285083770752\n",
            "total_loss a: 235.68285083770752\n",
            "total_loss b: 240.8985629081726\n",
            "total_loss a: 240.8985629081726\n",
            "total_loss b: 246.14648532867432\n",
            "total_loss a: 246.14648532867432\n",
            "total_loss b: 251.33113956451416\n",
            "total_loss a: 251.33113956451416\n",
            "total_loss b: 256.6172733306885\n",
            "total_loss a: 256.6172733306885\n",
            "total_loss b: 261.7592568397522\n",
            "total_loss a: 261.7592568397522\n",
            "total_loss b: 267.3152666091919\n",
            "total_loss a: 267.3152666091919\n",
            "total_loss b: 272.71225214004517\n",
            "total_loss a: 272.71225214004517\n",
            "total_loss b: 278.08743381500244\n",
            "total_loss a: 278.08743381500244\n",
            "total_loss b: 283.4765467643738\n",
            "total_loss a: 283.4765467643738\n",
            "total_loss b: 289.0277729034424\n",
            "total_loss a: 289.0277729034424\n",
            "total_loss b: 294.42363595962524\n",
            "total_loss a: 294.42363595962524\n",
            "total_loss b: 299.6752338409424\n",
            "total_loss a: 299.6752338409424\n",
            "total_loss b: 305.0144052505493\n",
            "total_loss a: 305.0144052505493\n",
            "Epoch 2 loss: 5.350672064156368\n",
            "total_loss b: 0\n",
            "total_loss a: 0\n",
            "total_loss b: 5.519631385803223\n",
            "total_loss a: 5.519631385803223\n",
            "total_loss b: 10.797945499420166\n",
            "total_loss a: 10.797945499420166\n",
            "total_loss b: 16.057531356811523\n",
            "total_loss a: 16.057531356811523\n",
            "total_loss b: 21.34705400466919\n",
            "total_loss a: 21.34705400466919\n",
            "total_loss b: 26.71566915512085\n",
            "total_loss a: 26.71566915512085\n",
            "total_loss b: 32.084017753601074\n",
            "total_loss a: 32.084017753601074\n",
            "total_loss b: 37.66896724700928\n",
            "total_loss a: 37.66896724700928\n",
            "total_loss b: 43.33528470993042\n",
            "total_loss a: 43.33528470993042\n",
            "total_loss b: 48.60894298553467\n",
            "total_loss a: 48.60894298553467\n",
            "total_loss b: 54.188374042510986\n",
            "total_loss a: 54.188374042510986\n",
            "total_loss b: 59.89895677566528\n",
            "total_loss a: 59.89895677566528\n",
            "total_loss b: 65.13323926925659\n",
            "total_loss a: 65.13323926925659\n",
            "total_loss b: 70.59936046600342\n",
            "total_loss a: 70.59936046600342\n",
            "total_loss b: 76.01575374603271\n",
            "total_loss a: 76.01575374603271\n",
            "total_loss b: 81.61662101745605\n",
            "total_loss a: 81.61662101745605\n",
            "total_loss b: 86.96740579605103\n",
            "total_loss a: 86.96740579605103\n",
            "total_loss b: 92.24275398254395\n",
            "total_loss a: 92.24275398254395\n",
            "total_loss b: 97.53968143463135\n",
            "total_loss a: 97.53968143463135\n",
            "total_loss b: 102.9278039932251\n",
            "total_loss a: 102.9278039932251\n",
            "total_loss b: 108.14704895019531\n",
            "total_loss a: 108.14704895019531\n",
            "total_loss b: 113.43845319747925\n",
            "total_loss a: 113.43845319747925\n",
            "total_loss b: 118.60302543640137\n",
            "total_loss a: 118.60302543640137\n",
            "total_loss b: 123.7576994895935\n",
            "total_loss a: 123.7576994895935\n",
            "total_loss b: 128.92107582092285\n",
            "total_loss a: 128.92107582092285\n",
            "total_loss b: 134.17547369003296\n",
            "total_loss a: 134.17547369003296\n",
            "total_loss b: 139.60845947265625\n",
            "total_loss a: 139.60845947265625\n",
            "total_loss b: 145.0088701248169\n",
            "total_loss a: 145.0088701248169\n",
            "total_loss b: 150.29943323135376\n",
            "total_loss a: 150.29943323135376\n",
            "total_loss b: 155.67306661605835\n",
            "total_loss a: 155.67306661605835\n",
            "total_loss b: 160.95866584777832\n",
            "total_loss a: 160.95866584777832\n",
            "total_loss b: 166.10771131515503\n",
            "total_loss a: 166.10771131515503\n",
            "total_loss b: 171.24466133117676\n",
            "total_loss a: 171.24466133117676\n",
            "total_loss b: 176.30122232437134\n",
            "total_loss a: 176.30122232437134\n",
            "total_loss b: 181.74039125442505\n",
            "total_loss a: 181.74039125442505\n",
            "total_loss b: 187.08749532699585\n",
            "total_loss a: 187.08749532699585\n",
            "total_loss b: 192.34518909454346\n",
            "total_loss a: 192.34518909454346\n",
            "total_loss b: 197.70365476608276\n",
            "total_loss a: 197.70365476608276\n",
            "total_loss b: 203.16472911834717\n",
            "total_loss a: 203.16472911834717\n",
            "total_loss b: 208.72446250915527\n",
            "total_loss a: 208.72446250915527\n",
            "total_loss b: 214.021812915802\n",
            "total_loss a: 214.021812915802\n",
            "total_loss b: 219.44675064086914\n",
            "total_loss a: 219.44675064086914\n",
            "total_loss b: 224.52970790863037\n",
            "total_loss a: 224.52970790863037\n",
            "total_loss b: 229.9168839454651\n",
            "total_loss a: 229.9168839454651\n",
            "total_loss b: 235.15951204299927\n",
            "total_loss a: 235.15951204299927\n",
            "total_loss b: 240.36895418167114\n",
            "total_loss a: 240.36895418167114\n",
            "total_loss b: 245.6154818534851\n",
            "total_loss a: 245.6154818534851\n",
            "total_loss b: 250.79696559906006\n",
            "total_loss a: 250.79696559906006\n",
            "total_loss b: 256.0763339996338\n",
            "total_loss a: 256.0763339996338\n",
            "total_loss b: 261.21502113342285\n",
            "total_loss a: 261.21502113342285\n",
            "total_loss b: 266.75912523269653\n",
            "total_loss a: 266.75912523269653\n",
            "total_loss b: 272.1508231163025\n",
            "total_loss a: 272.1508231163025\n",
            "total_loss b: 277.52242851257324\n",
            "total_loss a: 277.52242851257324\n",
            "total_loss b: 282.90385246276855\n",
            "total_loss a: 282.90385246276855\n",
            "total_loss b: 288.44570684432983\n",
            "total_loss a: 288.44570684432983\n",
            "total_loss b: 293.8320665359497\n",
            "total_loss a: 293.8320665359497\n",
            "total_loss b: 299.0765151977539\n",
            "total_loss a: 299.0765151977539\n",
            "total_loss b: 304.40621757507324\n",
            "total_loss a: 304.40621757507324\n",
            "Epoch 3 loss: 5.34010568980513\n",
            "total_loss b: 0\n",
            "total_loss a: 0\n",
            "total_loss b: 5.5088934898376465\n",
            "total_loss a: 5.5088934898376465\n",
            "total_loss b: 10.781230449676514\n",
            "total_loss a: 10.781230449676514\n",
            "total_loss b: 16.037019729614258\n",
            "total_loss a: 16.037019729614258\n",
            "total_loss b: 21.319859981536865\n",
            "total_loss a: 21.319859981536865\n",
            "total_loss b: 26.680829524993896\n",
            "total_loss a: 26.680829524993896\n",
            "total_loss b: 32.03939247131348\n",
            "total_loss a: 32.03939247131348\n",
            "total_loss b: 37.61542844772339\n",
            "total_loss a: 37.61542844772339\n",
            "total_loss b: 43.26827335357666\n",
            "total_loss a: 43.26827335357666\n",
            "total_loss b: 48.528767585754395\n",
            "total_loss a: 48.528767585754395\n",
            "total_loss b: 54.095834255218506\n",
            "total_loss a: 54.095834255218506\n",
            "total_loss b: 59.78473377227783\n",
            "total_loss a: 59.78473377227783\n",
            "total_loss b: 64.99852848052979\n",
            "total_loss a: 64.99852848052979\n",
            "total_loss b: 70.4466233253479\n",
            "total_loss a: 70.4466233253479\n",
            "total_loss b: 75.84468126296997\n",
            "total_loss a: 75.84468126296997\n",
            "total_loss b: 81.42400598526001\n",
            "total_loss a: 81.42400598526001\n",
            "total_loss b: 86.75161266326904\n",
            "total_loss a: 86.75161266326904\n",
            "total_loss b: 92.00139904022217\n",
            "total_loss a: 92.00139904022217\n",
            "total_loss b: 97.26615381240845\n",
            "total_loss a: 97.26615381240845\n",
            "total_loss b: 102.61641597747803\n",
            "total_loss a: 102.61641597747803\n",
            "total_loss b: 107.78465843200684\n",
            "total_loss a: 107.78465843200684\n",
            "total_loss b: 113.02011060714722\n",
            "total_loss a: 113.02011060714722\n",
            "total_loss b: 118.11475610733032\n",
            "total_loss a: 118.11475610733032\n",
            "total_loss b: 123.19763612747192\n",
            "total_loss a: 123.19763612747192\n",
            "total_loss b: 128.26926803588867\n",
            "total_loss a: 128.26926803588867\n",
            "total_loss b: 133.4395751953125\n",
            "total_loss a: 133.4395751953125\n",
            "total_loss b: 138.77105474472046\n",
            "total_loss a: 138.77105474472046\n",
            "total_loss b: 144.05256605148315\n",
            "total_loss a: 144.05256605148315\n",
            "total_loss b: 149.2236909866333\n",
            "total_loss a: 149.2236909866333\n",
            "total_loss b: 154.46090173721313\n",
            "total_loss a: 154.46090173721313\n",
            "total_loss b: 159.60782194137573\n",
            "total_loss a: 159.60782194137573\n",
            "total_loss b: 164.58990383148193\n",
            "total_loss a: 164.58990383148193\n",
            "total_loss b: 169.53973054885864\n",
            "total_loss a: 169.53973054885864\n",
            "total_loss b: 174.38353967666626\n",
            "total_loss a: 174.38353967666626\n",
            "total_loss b: 179.63761568069458\n",
            "total_loss a: 179.63761568069458\n",
            "total_loss b: 184.77037382125854\n",
            "total_loss a: 184.77037382125854\n",
            "total_loss b: 189.7766432762146\n",
            "total_loss a: 189.7766432762146\n",
            "total_loss b: 194.90615892410278\n",
            "total_loss a: 194.90615892410278\n",
            "total_loss b: 200.1265778541565\n",
            "total_loss a: 200.1265778541565\n",
            "total_loss b: 205.44408321380615\n",
            "total_loss a: 205.44408321380615\n",
            "total_loss b: 210.4100260734558\n",
            "total_loss a: 210.4100260734558\n",
            "total_loss b: 215.5489387512207\n",
            "total_loss a: 215.5489387512207\n",
            "total_loss b: 220.4023060798645\n",
            "total_loss a: 220.4023060798645\n",
            "total_loss b: 225.53208541870117\n",
            "total_loss a: 225.53208541870117\n",
            "total_loss b: 230.42998027801514\n",
            "total_loss a: 230.42998027801514\n",
            "total_loss b: 235.43531703948975\n",
            "total_loss a: 235.43531703948975\n",
            "total_loss b: 240.44115734100342\n",
            "total_loss a: 240.44115734100342\n",
            "total_loss b: 245.32568359375\n",
            "total_loss a: 245.32568359375\n",
            "total_loss b: 250.41469764709473\n",
            "total_loss a: 250.41469764709473\n",
            "total_loss b: 255.23629760742188\n",
            "total_loss a: 255.23629760742188\n",
            "total_loss b: 260.4514055252075\n",
            "total_loss a: 260.4514055252075\n",
            "total_loss b: 265.62353706359863\n",
            "total_loss a: 265.62353706359863\n",
            "total_loss b: 270.6466488838196\n",
            "total_loss a: 270.6466488838196\n",
            "total_loss b: 275.8097767829895\n",
            "total_loss a: 275.8097767829895\n",
            "total_loss b: 281.0999217033386\n",
            "total_loss a: 281.0999217033386\n",
            "total_loss b: 286.2683939933777\n",
            "total_loss a: 286.2683939933777\n",
            "total_loss b: 291.1697669029236\n",
            "total_loss a: 291.1697669029236\n",
            "total_loss b: 296.2055735588074\n",
            "total_loss a: 296.2055735588074\n",
            "Epoch 4 loss: 5.193425951332881\n",
            "total_loss b: 0\n",
            "total_loss a: 0\n",
            "total_loss b: 5.277825355529785\n",
            "total_loss a: 5.277825355529785\n",
            "total_loss b: 10.274472713470459\n",
            "total_loss a: 10.274472713470459\n",
            "total_loss b: 15.22952127456665\n",
            "total_loss a: 15.22952127456665\n",
            "total_loss b: 20.202805042266846\n",
            "total_loss a: 20.202805042266846\n",
            "total_loss b: 25.273248195648193\n",
            "total_loss a: 25.273248195648193\n",
            "total_loss b: 30.32765483856201\n",
            "total_loss a: 30.32765483856201\n",
            "total_loss b: 35.63740682601929\n",
            "total_loss a: 35.63740682601929\n",
            "total_loss b: 41.0287299156189\n",
            "total_loss a: 41.0287299156189\n",
            "total_loss b: 45.93739128112793\n",
            "total_loss a: 45.93739128112793\n",
            "total_loss b: 51.18053436279297\n",
            "total_loss a: 51.18053436279297\n",
            "total_loss b: 56.581424713134766\n",
            "total_loss a: 56.581424713134766\n",
            "total_loss b: 61.377928256988525\n",
            "total_loss a: 61.377928256988525\n",
            "total_loss b: 66.49642419815063\n",
            "total_loss a: 66.49642419815063\n",
            "total_loss b: 71.56081771850586\n",
            "total_loss a: 71.56081771850586\n",
            "total_loss b: 76.81517791748047\n",
            "total_loss a: 76.81517791748047\n",
            "total_loss b: 81.84085464477539\n",
            "total_loss a: 81.84085464477539\n",
            "total_loss b: 86.7169189453125\n",
            "total_loss a: 86.7169189453125\n",
            "total_loss b: 91.60272979736328\n",
            "total_loss a: 91.60272979736328\n",
            "total_loss b: 96.62802648544312\n",
            "total_loss a: 96.62802648544312\n",
            "total_loss b: 101.47329902648926\n",
            "total_loss a: 101.47329902648926\n",
            "total_loss b: 106.38780546188354\n",
            "total_loss a: 106.38780546188354\n",
            "total_loss b: 111.07252311706543\n",
            "total_loss a: 111.07252311706543\n",
            "total_loss b: 115.79662847518921\n",
            "total_loss a: 115.79662847518921\n",
            "total_loss b: 120.46922779083252\n",
            "total_loss a: 120.46922779083252\n",
            "total_loss b: 125.40746879577637\n",
            "total_loss a: 125.40746879577637\n",
            "total_loss b: 130.50237369537354\n",
            "total_loss a: 130.50237369537354\n",
            "total_loss b: 135.54274368286133\n",
            "total_loss a: 135.54274368286133\n",
            "total_loss b: 140.39798021316528\n",
            "total_loss a: 140.39798021316528\n",
            "total_loss b: 145.37193298339844\n",
            "total_loss a: 145.37193298339844\n",
            "total_loss b: 150.28098964691162\n",
            "total_loss a: 150.28098964691162\n",
            "total_loss b: 155.04666662216187\n",
            "total_loss a: 155.04666662216187\n",
            "total_loss b: 159.8178310394287\n",
            "total_loss a: 159.8178310394287\n",
            "total_loss b: 164.46995973587036\n",
            "total_loss a: 164.46995973587036\n",
            "total_loss b: 169.50900840759277\n",
            "total_loss a: 169.50900840759277\n",
            "total_loss b: 174.43818616867065\n",
            "total_loss a: 174.43818616867065\n",
            "total_loss b: 179.25838136672974\n",
            "total_loss a: 179.25838136672974\n",
            "total_loss b: 184.21766471862793\n",
            "total_loss a: 184.21766471862793\n",
            "total_loss b: 189.19824123382568\n",
            "total_loss a: 189.19824123382568\n",
            "total_loss b: 194.3074173927307\n",
            "total_loss a: 194.3074173927307\n",
            "total_loss b: 199.12426948547363\n",
            "total_loss a: 199.12426948547363\n",
            "total_loss b: 204.0828561782837\n",
            "total_loss a: 204.0828561782837\n",
            "total_loss b: 208.76943826675415\n",
            "total_loss a: 208.76943826675415\n",
            "total_loss b: 213.71754598617554\n",
            "total_loss a: 213.71754598617554\n",
            "total_loss b: 218.46399593353271\n",
            "total_loss a: 218.46399593353271\n",
            "total_loss b: 223.25352478027344\n",
            "total_loss a: 223.25352478027344\n",
            "total_loss b: 228.05375480651855\n",
            "total_loss a: 228.05375480651855\n",
            "total_loss b: 232.7637267112732\n",
            "total_loss a: 232.7637267112732\n",
            "total_loss b: 237.61991691589355\n",
            "total_loss a: 237.61991691589355\n",
            "total_loss b: 242.28006553649902\n",
            "total_loss a: 242.28006553649902\n",
            "total_loss b: 247.36442136764526\n",
            "total_loss a: 247.36442136764526\n",
            "total_loss b: 252.29889726638794\n",
            "total_loss a: 252.29889726638794\n",
            "total_loss b: 257.18621826171875\n",
            "total_loss a: 257.18621826171875\n",
            "total_loss b: 262.1055736541748\n",
            "total_loss a: 262.1055736541748\n",
            "total_loss b: 267.0849938392639\n",
            "total_loss a: 267.0849938392639\n",
            "total_loss b: 271.9882779121399\n",
            "total_loss a: 271.9882779121399\n",
            "total_loss b: 276.7048840522766\n",
            "total_loss a: 276.7048840522766\n",
            "total_loss b: 281.56957054138184\n",
            "total_loss a: 281.56957054138184\n",
            "Epoch 5 loss: 4.937842558170187\n"
          ]
        }
      ],
      "source": [
        "# Preprocess the training data\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')\n",
        "# train_data = [\" 87 97 105 117 123 132 154 176 192\", \" 100 98 106 118 124 131 156 179 193\", \" 102 99 108 113 129 132 156 178 199\"]\n",
        "# input_ids = [tokenizer.encode(text) for text in train_data\n",
        "\n",
        "# max_length: 864 measurements = 12 measurements/hour * 24 hours/day * 3 days\n",
        "# Open the file in read mode\n",
        "input_ids = []\n",
        "with open('data.txt', 'r') as file:\n",
        "    # Loop over each line in the file\n",
        "    for line in file:\n",
        "        line = ' ' + line # Add a single space to the first measurement\n",
        "        input_ids.append(tokenizer.encode(line, max_length = 864, truncation=True ))\n",
        "\n",
        "inputs = torch.tensor(input_ids)\n",
        "# print(inputs)\n",
        "\n",
        "'''\n",
        "# Set up the GPT-2 model configuration\n",
        "model_config = GPT2Config(\n",
        "    vocab_size=len(tokenizer),\n",
        "    n_embd=300,\n",
        "    n_layer=36,\n",
        "    n_head=12,\n",
        "    attn_pdrop=0.5,\n",
        "    resid_pdrop=0.5,\n",
        "    initializer_range=0.02,\n",
        "    repetition_penalty=1,  # No penalty\n",
        "    temperature = 0\n",
        ")\n",
        "'''\n",
        "\n",
        "# GPT2-XL\n",
        "model_config = GPT2Config(\n",
        "    vocab_size=50257,\n",
        "    n_positions=2048,\n",
        "    n_ctx=2048,\n",
        "    n_embd=1600,\n",
        "    n_layer=48,\n",
        "    n_head=25,\n",
        "    resid_pdrop=0.1,\n",
        "    embd_pdrop=0.1,\n",
        "    attn_pdrop=0.1,\n",
        "    layer_norm_epsilon=1e-5)\n",
        "\n",
        "\n",
        "# Set up the GPT-2 model with language modeling head\n",
        "model = GPT2LMHeadModel(model_config)\n",
        "\n",
        "# Train the model\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for input_seq in inputs:\n",
        "        print (f'total_loss b: {total_loss}')\n",
        "        optimizer.zero_grad()\n",
        "        print (f'total_loss a: {total_loss}')\n",
        "        input_seq = input_seq.unsqueeze(0)  # Reshape to 2D tensor\n",
        "        outputs = model(input_seq[:, :-1], labels=input_seq[:, 1:])\n",
        "        #print (f'outputs {outputs}')\n",
        "        loss = outputs.loss\n",
        "        # print (f'loss: {loss}')\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1} loss: {total_loss / len(inputs)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode (inputs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "7u1ItwkmrQQy",
        "outputId": "cd5eb278-f704-4d49-b999-b13e2008f804"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' 160 160 142 133 128 128 126 119 113 106 103 101 100 101 102 103 103 100 98 118 127 129 131 133 133 133 133 132 131 130 129 128 128 127 128 129 129 129 128 125 122 121 119 120 120 120 121 122 122 124 125 128 136 138 140 139 137 135 132 126 118 114 111 110 113 114 113 111 108 109 112 112 111 109 104 102 101 100 99 97 95 94 93 93 92 93 96 95 92 91 92 98 157 168 166 165 158 148 137 135 132 126 119 117 114 114 109 99 90 84 80 79 80 82 83 83 84 84 86 87 85 85 88 93 95 95 95 95 99 106 105 105 104 101 99 94 89 93 96 97 87 83 84 87 96 110 128 141 152 166 180 187 199 206 208 213 209 217 220 216 213 209 202 194 189 186 181 177 170 164 165 161 156 154 150 148 146 141 132 121 116 109 103 100 94 90 96 101 104 98 91 99 109 124 133 136 136 131 129 139 143 152 174 184 191 197 201 203 204 205 203 199 194 187 177 165 152 142 135 128 123 126 132 137 136 133 128 129 135 142 145 145 139 128 122 117 111 109 106 105 107 113 129 134 133 129 121 118 114 109 107 107 104 99 99 102 110 120 127 127 127 132 143 153 157 159 155 149 143 167 176 185 196 209 219 222 231 237 245 255 263 270 278 278 281 276 271 265 252 240 229 218 205 192 177 164 152 143 134 123 116 109 105 107 113 118 121 123 124 112 103 106 115 122 124 121 122 118 112 113 124 136 156 174 197 217 228 245 260 272 282 287 291 289 283 279 278 280 281 279 275 271 270 267 262 260 259 256 261 268 268 266 263 261 260 258 256 255 253 252 248 240 242 241 238 230 231 231 226 218 209 200 190 193 192 189 189 175 184 182 179 176 172 165 160 154 143 139 137 135 133 127 119 119 123 127 127 125 121 118 118 120 122 123 122 121 119 118 118 118 118 117 117 117 117 116 113 113 115 117 119 119 119 118 121 126 129 129 127 122 111 115 123 142 170 189 207 226 247 259 278 302 331 350 366 394 400 400 260 296 300 293 276 267 255 246 235 226 229 227 226 221 215 216 215 209 208 208 210 217 222 221 221 222 226 236 235 240 249 259 265 269 270 266 258 251 294 311 307 309 311 307 291 274 264 255 246 243 237 225 217 209 200 181 170 158 148 138 128 119 110 102 95 95 92 86 82 79 74 72 70 64 56 49 46 45 46 48 74 86 102 103 102 110 117 123 135 159 175 190 189 193 195 184 177 170 167 166 164 159 153 142 137 130 123 124 125 126 124 120 115 112 109 109 111 112 117 117 118 117 113 108 103 105 106 112 126 133 140 147 143 137 136 136 138 137 133 128 121 116 119 121 123 121 115 109 106 107 109 112 114 113 112 113 115 114 113 112 113 113 111 106 108 118 124 126 118 111 105 108 111 112 109 102 98 97 95 95 96 96 96 95 93 91 91 96 100 106 108 114 136 161 182 202 226 248 264 285 303 310 323 326 328 320 312 309 305 309 312 312 311 309 288 278 273 270 264 249 239 230 219 205 193 186 192 205 218 229 236 242 239 232 224 218 214 211 215 219 221 218 210 202 194 188 180 170 161 156 149 141 134 126 120 117 116 112 106 102 97 92 88 87 93 102 113 126 136 144 150 150 145 139 132 117 105 96 88 81 76 72 69 67 66 66 66 69 74 89 112 128 141 151 159 166 172 181 193 206 218 221 231 239 245 250 257 266 267 272 271 269 271 274 278 286 291 291 286 281 275 267 262 263 267 272 275 276 274 267 248 253 257 252 249 255 264 273 278 278 273 270 268 263 250 240 235 231 228 225 220 214 212 207 201 205 202 192 183 175 174 170 162 152 146 141 134 131 130 129 126 119 117 121 122 122 121 116 111 112 118 127 131 133 134 139 137 136 143 159 170 175 169 159 141 126 120 116 114 115 115 115 130 145 160 164 166 167 162 158 161 172 179 179 177'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a prediction\n",
        "# 136 136 131 129 139 143 152 174 184 191 197 201 203 204 205 203 199 194 187 177 165 152 142 135\n",
        "text = \" 136 136 131 129 139 143 152\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "outputs = model.generate(input_ids, max_length=20)\n",
        "predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(predicted_text)\n",
        "#epoch = 1: 136 136 131 129 139 143 152 174 184 117 117 117 117 117 117 132 117 117 117 117\n",
        "#epoch = 5 (start: 337.21 end:307.81 compute units: 29.4, approx. 2 hours)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUj48VOioe9n",
        "outputId": "80fb6700-d180-4f17-b1e3-769a258eb00f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 136 136 131 129 139 143 152 163 157 163 157 164 163 163 163 163 163 163 163 163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "337.21 - 307.81"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6KC8AZ_UpKv",
        "outputId": "c80554dd-ee8c-4768-e8b8-8b218d96436a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29.399999999999977"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}